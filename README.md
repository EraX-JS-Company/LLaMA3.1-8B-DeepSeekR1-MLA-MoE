# EraX: Reimagine LLaMA 3.1 with DeepSeek's Innovation – and We Need Your Help!

At EraX, curiosity drives us. We've taken the groundbreaking LLaMA 3.1 8B model and engineered a revolutionary transformation, selectively integrating DeepSeek R1's cutting-edge Multi-Head Latent Attention (MLA) and Mixture of Experts (MoE) layers.

We're excited to share the code and raw model – refined with insights from Claude Sonnet 3.7 – enabling you to:

*   **Transform LLaMA 3.1:** Seamlessly convert specific layers of LLaMA 3.1-8B to DeepSeek R1's advanced MLA and MoE architecture.
*   **Experience the Future:** Reload and rigorously test the newly architected model, unlocking its potential.
*   **Unlock New Frontiers:** Leverage our continual pretraining code, powered by FSDP (or DDP for BitAndBytes 8-bit optimization), to push the boundaries of model performance.

## The Challenge: Unleashing the Power

While we've built the foundation, we need the resources to truly unleash this hybrid model's capabilities. Specifically, we're seeking funding – GPU compute – to embark on the crucial stages of pretraining and fine-tuning.

## The Vision: Open-Source AI for All

We believe in democratizing AI. If you're in a position to contribute compute resources and help us train this converted model, we implore you to do so. More importantly, we pledge to open-source the fully trained model on Hugging Face so the entire community can benefit from its innovation.

Join us in shaping the future of AI. Let's transform LLaMA 3.1 together!

**You can access the raw model here: [EraX-LLaMA3.1-8B-DeepSeekR1-MLA-MoE-Raw](https://huggingface.co/erax-ai/EraX-LLaMA3.1-8B-DeepSeekR1-MLA-MoE-Raw/tree/main)**

Good luck, and we look forward to seeing what you create!

The EraX Team.
